#MultiHashMap

When I heard about collision management in the Algorithms and Data Structures subject, I thought this would be a good hash map implementation.

I did a similar implementation to separate chaining, but instead of doing a chain in every one of the hashes, we do a chain of hash tables of the same size. This way, we have a similar cost to separate chaining, but we avoid iterating on lists because we can check in what hash map our key is through a simple division: key/hash_table_capacity. This makes it quicker than any other system to find any element at the hash map because any other system needs to iterate through the hash map or a list.

I achieved a hash map system that allows users to store data without having a big cost at insert, update, consult, or erase (most times O(1)).
  Cost analysis:
    Insertion: Most times it will cost O(1), but as it uses a vector that can grow, there are some times that hash map insertion will cost O(n).
    Update: As it uses a vector position and a key, the cost of arriving at the right value will be O(1), and updating the value too, so the final cost is also O(1).
    Erase: As it uses a vector position and a key, the cost of arriving at the right value will be O(1), erasing the element too, so the final cost is also O(1).
    Find: As it uses a vector position and a key, the cost of arriving at the right value will be O(1).

    Memory cost: As it uses a vector of simple hash maps, this will be so big compared to other implementations. To be specific, the memory cost will be O(nÂ²).

In conclusion, I developed a really quick consulting and operating hash map, with a very economic operating cost of O(1) in most cases, but with a very expensive memory cost: O(n).
